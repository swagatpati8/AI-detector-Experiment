Document,Human OR AI,Assignment
"  The purpose and goal of the COMPAS algorithm was to determine the likelihood that someone would commit a crime in the future, or be a repeat offender. The issue ended up simply being the rampant mistakes present in the algorithm. For example, one aspect of the algorithm that provoked attention was the misclassification of individuals along racial lines. White people were mistakenly categorized as unlikely to be repeat offenders about as often as black people were mistakenly categorized as likely to be repeat offenders, which consisted of almost half of total individuals of each race in each case. This was all done without race being taken in as a factor in the actual algorithm, so it was clear that whatever questions posed to the individual (the method of information input) or the weighing of them was not ideal. This is only one example of the inaccuracy of the algorithm. To fix the problem, an entire analysis must be done on the nature of crime and factors that go into people becoming repeat offenders, and such information should be analyzed without allowing for input from the individuals themselves, based on purely objective factors. Fait should not be placed in an algorithm such as COMPAS, even without knowing the level of inaccuracy from the beginning. ",Human,COMPAS
"  The COMPAS algorithm was created to help judges assess the likelihood that a defendant would reoffend. Ideally, it was supposed to make decisions like bail and sentencing more fair and objective. However, as ProPublica’s investigation shows, the algorithm often did the opposite. It turned out to be only slightly better than guessing, and it produced serious racial disparities: Black defendants were much more likely to be wrongly labeled high-risk, while white defendants were more often labeled low-risk even when they weren't.  This suggests that instead of removing human bias, COMPAS reflected and reinforced existing inequalities, likely because it was trained on biased historical data. One way to fix these problems would be to make the algorithm more transparent so that independent researchers could evaluate and improve it. Developers should also rethink what kinds of data they use, making sure not to rely on variables that act as stand-ins for race. In general, using simpler models that people can actually interpret, and combining them with human judgment, might prevent some of the unfair outcomes. Algorithms have potential to make the justice system better, but only if they are built and used carefully. ",AI,COMPAS
"  In the first article shows how things like selectively reporting variables or stopping data collection once significance is reached are capable of producing false positives. In the article it proposed a solution where a set of six disclosure requirements for authors and guidelines for reviewers, focusing on transparency. In the Asterisk article, they reflect on how effective those changes have been. This would consist of recommendations becoming more foundation to the open science movement. Preregistration, sharing datasets, and etc. have become more common. These changes have been shifting incentives from simply publishing flashy results to more logical. My favorite example is from the first article which shows that listening to the Beatles have made participants younger. It doesn't make sense but it's a funny thing to think about. An improvement that wasn't discussed would be embedding replication as a part of funding. So, it could normalize the practice and make science more self-correcting by default. ",Human,Replication
"  The Asterisk article highlights how psychology and other social sciences have responded to the replication crisis over the past decade. It shows that while the original academic article by Simmons, Nelson, and Simonsohn in 2011 helped spark a necessary conversation about “researcher degrees of freedom” and p-hacking, implementing those reforms in real life has been slow and uneven. Suggestions like pre-registration, sharing data, and increasing sample sizes have been adopted more widely, but the culture of publication pressure and novelty-seeking still persists. The article suggests that systemic change is still a work in progress.  One of my favorite examples is from the original Simmons et al. paper, where they show how manipulating seemingly minor aspects of data collection and analysis (like when to stop collecting data) can lead to statistically significant but completely meaningless results—like proving that listening to a certain song can make you younger. This absurd example effectively illustrated how easy it is to “find” significance where none exists, and how flawed the process had become.  One suggestion I would add is to change academic incentives—journals should be more willing to publish null results and replication studies. Right now, career advancement is often tied to novel findings, which discourages careful, transparent science. Reforming those incentives could go a long way toward real, lasting improvement. ",AI,Replication
"  The ""Ratings: Deep Dive"" and ""Compact Districts Aren't Fair"" articles provided a fascinating perspective on the challenges of creating electoral maps. Something that struck me as particularly surprising was the way in which the first article demonstrated that rating systems rely on subjective views, particularly regarding compactness and fairness. I once thought compact meant that it was equal. However, the second article showed me that well-drawn districts can accidentally help certain political parties because of where people live.  If I were redistricting, I would rethink placing too great a priority on compactness. Instead, I would place greater emphasis on juggling competitiveness and fairness, making sure voter choices are represented fairly, not simply creating districts that look good. I agree that compact does not always mean fair. I was surprised that compact districts in certain situations would create disparities more dramatic by packing alike voters together, thus leading to unfair results. It illustrates how we need to have a more sophisticated and multifaceted method to redistricting than simple measurements. In total, these readings illustrate just how complex and detailed we will need to be in order to create equal electoral districts. ",Human,DRA
"   After reading the “Ratings: Deep Dive” article, I was most surprised by how subjective and relative the rating system is. While the metrics—proportionality, competitiveness, minority representation, compactness, and splitting—are normalized to a 0–100 scale, the article emphasizes that these ratings are not absolute measures. They are designed to facilitate comparisons between maps within the same state, acknowledging that political geography varies significantly across states. This relativity challenges the notion of a universally “fair” map and underscores the complexity of redistricting.  In the “Compact Districts Aren’t Fair” piece, I agree with the argument that compactness does not inherently equate to fairness. The article illustrates that in states with politically homogeneous geographies, compact districts can still result in disproportionate representation due to the clustering of like-minded voters. This insight reveals that prioritizing compactness can inadvertently perpetuate partisan advantages, highlighting the need for a more nuanced approach to evaluating district fairness.  If I were to make a different choice in the rating system, I would advocate for greater emphasis on proportionality and minority representation. These factors more directly address the equitable reflection of voters' preferences and the protection of minority groups' electoral influence, which are fundamental to a representative democracy. ",AI,DRA
"Overall, the mechanism choice will either bring a lower or higher rate of truth telling. For example, the Boston Mechanism has lower rates of truth telling than Top Trading Cycles and Deferred Acceptance in the majority of the studies conducted. However, the latter two mechanisms are harder to compare. According to the the section, beyond the mechanisms, the environment in which they are used and the amount of experience with the mechanisms themselves.   One interesting insight from Section 3.2 is that Deferred Acceptance will generally have better results with more repetition. However, with the Boston Mechanism, there was no evidence of repetition leading to better results.   I would look at a few things. The first priority is the truth telling rates. Boston Mechanism probably is not the best one to choose, so the debate is between the Deferred Acceptance and Top Trading Cycles. Secondly, I'd look at the efficiency of the mechanisms. ",Human,Matching
"  In section 3.1, the paper talks about how different mechanisms can impact whether people are honest about their preferences. Basically, when the system encourages or forces people to strategize, they’re less likely to tell the truth. If people think they can “game” the system to get a better outcome, they will. On the other hand, if a mechanism is strategy-proof (like the Deferred Acceptance mechanism), then people are more likely to be honest because there's no advantage to lying.  I also skimmed section 4, which looked at experiments related to school choice. One thing I found interesting was how even small changes to the rules of the system can change people’s behavior a lot. For example, when people are confused about how the system works, they tend to default to what seems safe instead of what they really want. That can lead to worse outcomes for everyone.  So if I were choosing a mechanism for school choice, I’d probably go with something that’s easy to understand and strategy-proof, like Deferred Acceptance. It makes things more fair and reduces the pressure on families to “play the game” just right to get a good school. ",AI,Matching
