{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# need code for something called as an emebedding model on top\n",
        "def arguement(int):\n",
        "  if parse.args == f{string:\"print samn\"}"
      ],
      "metadata": {
        "id": "iFHb-3M-40D6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPA6boveyCtT"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install datasets        # or:  !pip install datasets  (in a notebook)\n",
        "!pip install -U \"gcsfs==2024.12.0\"\n",
        "\n",
        "import sys, json, random, argparse, os\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "\n",
        "# ---------- helpers --------------------------------------------------------- #\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def build_dataset(csv_path: str) -> DatasetDict:\n",
        "    \"\"\"Read a CSV (columns: text,label) → 60 / 40 train/test split.\"\"\"\n",
        "    data = load_dataset(\"csv\", data_files=csv_path)[\"train\"]\n",
        "    splits = data.train_test_split(test_size=0.4, seed=42, stratify_by_column=\"label\")\n",
        "    return DatasetDict(train=splits[\"train\"], test=splits[\"test\"])\n",
        "\n",
        "\n",
        "def tokenize_function(examples, tokenizer):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
        "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
        "\n",
        "\n",
        "# ---------- training -------------------------------------------------------- #\n",
        "def train(csv_path: str, out_dir: str, epochs: int = 3, batch: int = 8):\n",
        "    set_seed()\n",
        "    ds = build_dataset(csv_path)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    ds = ds.map(lambda x: tokenize_function(x, tokenizer), batched=True,\n",
        "                remove_columns=[\"text\"])\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-uncased\", num_labels=2\n",
        "    )\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=out_dir,\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=batch,\n",
        "        per_device_eval_batch_size=batch,\n",
        "        num_train_epochs=epochs,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        weight_decay=0.01,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model,\n",
        "        args,\n",
        "        train_dataset=ds[\"train\"],\n",
        "        eval_dataset=ds[\"test\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    print(\"\\nFINAL METRICS:\", json.dumps(trainer.evaluate(), indent=2))\n",
        "    trainer.save_model(out_dir)\n",
        "    tokenizer.save_pretrained(out_dir)\n",
        "\n",
        "\n",
        "# ---------- inference ------------------------------------------------------- #\n",
        "def predict_one(text: str, model_dir: str):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    prob_ai = torch.softmax(logits, dim=-1)[0, 0].item()   # class 0 = AI\n",
        "    label = \"AI‑GENERATED\" if prob_ai >= 0.5 else \"HUMAN\"\n",
        "    print(f\"\\nResult: {label}   (prob_AI = {prob_ai:.3f})\")\n",
        "\n",
        "\n",
        "# ---------- CLI ------------------------------------------------------------- #\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"BERT AI‑text cheat detector\")\n",
        "    sub = parser.add_subparsers(dest=\"cmd\", required=True)\n",
        "\n",
        "    t = sub.add_parser(\"train\", help=\"fine‑tune on a labelled CSV\")\n",
        "    t.add_argument(\"csv_path\"), t.add_argument(\"out_dir\")\n",
        "    t.add_argument(\"--epochs\", type=int, default=3)\n",
        "    t.add_argument(\"--batch\", type=int, default=8)\n",
        "\n",
        "    p = sub.add_parser(\"predict\", help=\"classify a single passage\")\n",
        "    p.add_argument(\"text\"), p.add_argument(\"model_dir\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.cmd == \"train\":\n",
        "        train(args.csv_path, args.out_dir, args.epochs, args.batch)\n",
        "    else:\n",
        "        predict_one(args.text, args.model_dir)\n"
      ]
    }
  ]
}