Document,Human OR AI,Assignment
"  I have read about this issue in other classes as well - I find this to be interesting as judges and the court system can be biased as well and algorithms can be reflected of the people writing it. I also think that it's difficult to reduce the risk of committing a crime to a series of questions. Some interesting concerns were brought up as well, such as Zilly bringing up how his past isn't representative of his current situation. Another interesting issue I read in the comments below was how some questions were redundant adding more weight to certain issues in the final assessment. So questions on a past in foster care, incarceration of immediate family members, and gang affiliation. While these questions aren't racially explicit, African Americans are more likely to be in certain categories (for example while making up only 11% of the population, African Americans make up 22% of the foster care system). While the solution to this issue isn't simple, refining the data collection process can be a step in the right direction - however it could be argued that it is morally incorrect for a machine to label the chance of someone committing a crime. ",Human,COMPAS
"  The COMPAS algorithm was designed to assess a defendant’s likelihood of reoffending and help guide decisions about bail, sentencing, and parole. The main goal was to bring objectivity and data-driven decisions into the criminal justice system by using predictive analytics. However, in practice, it reinforced existing biases rather than eliminating them.  According to the ProPublica analysis, COMPAS often predicted that Black defendants were more likely to reoffend than they actually were, while it underrated the risk for white defendants. In fact, Black defendants who didn’t reoffend were often classified as high-risk, while white defendants who did reoffend were frequently classified as low-risk. This reveals a serious racial bias in the algorithm’s predictions.  The problem seems to stem from the data the algorithm was trained on—data that reflects systemic inequalities in policing and sentencing. If historical data is biased, algorithms trained on it will simply learn and perpetuate those same patterns.  To fix this, one approach could be to audit the algorithm regularly and introduce fairness constraints or bias mitigation techniques during training. Also, more transparency in how the model works and what factors it uses could help prevent these issues. Relying less on “black-box” proprietary systems and involving independent researchers in evaluating these tools would be a step toward greater accountability. ",AI,COMPAS
"  The COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) algorithm was one of the more recent developments in court ruling and jail time assessments. This kind of algorithm is called a risk assessment. Their goals are to create a risk score on an offender who committed a crime. This score will then decide their entire future career: Their prison sentence and their criminal history. Their goal is to make this risk score as accurate and unbiased as possible. The problem with this particular algorithm and a lot of other risk assessment algorithms that no matter if the data points do not contain race in any way, because of other data like poverty, joblessness, and social marginalization that most assessments keep in. This is because this data increases the overall accuracy by a high margin. However, looking deeper into the results, One can find that the amount of highly risk scored African Americans who went on to commit no crimes was much higher than the wrongfully scored white Americans, and the amount of white Americans who were scored low who went on to commit crimes was much higher than that of African Americans. The only ways to possibly fix this problem from my knowledge would be to skew the data manually based on some data points to increase the accuracy on a sample dataset or to remove data which the model could potentially racially profile from and find other questions or data to add. ",Human,COMPAS
"   The ProPublica investigation into the COMPAS algorithm reveals significant concerns about algorithmic fairness in the criminal justice system. Despite not explicitly using race as a factor, COMPAS disproportionately assigned higher risk scores to Black defendants compared to white defendants with similar criminal histories. The analysis showed that Black defendants who did not reoffend were nearly twice as likely to be misclassified as high risk compared to their white counterparts (45% vs. 23%) .  This disparity underscores the issue of proxy variables in algorithmic decision-making. Factors like prior arrests and socioeconomic status, which are correlated with race due to systemic inequalities, can inadvertently introduce racial bias into algorithms. Moreover, the algorithm's overall predictive accuracy was moderate—correctly predicting recidivism 61% of the time and violent recidivism only 20% of the time .  The use of such algorithms in sentencing decisions raises ethical questions about transparency and accountability. Defendants often have limited recourse to challenge algorithmic assessments, especially when the proprietary nature of these tools obscures their inner workings. This lack of transparency can erode trust in the justice system and perpetuate existing disparities.  In conclusion, while algorithms like COMPAS aim to bring objectivity to sentencing, their implementation without rigorous oversight and transparency can exacerbate racial biases. It's imperative to critically assess and regulate these tools to ensure they promote fairness rather than undermine it. ",AI,COMPAS
"  To what extent did the suggestions made at the end of the academic article actually help in real life as portrayed by the article written a decade later? There has been some progress but it has not been fixed entirely, there has been awareness raised for this and new tools like open data and preregistration. While this does improve replication, the question of p-hacking is still evident as well as other bad practices.   What is your favorite example from either the acadmic paper or the article of a real life study affected by the replication crisis? I liked the cardboard box one, it was never replicated, and it was clear why because it wouldn't work the same!   What is one suggestion not mentioned that you think might help with the issues discussed in the replication crisis? One suggestion could be independent reviewing panels , maybe even with public stats/leaderboards that show the previous rankings and reputation of the scientists & researchers ",Human,Replication
"  The replication crisis revealed how deeply flawed many research practices were, and according to the Asterisk article, while major reforms have occurred, problems persist. Suggestions like preregistration and encouraging replication studies have helped: today, practices such as open data sharing, preregistered hypotheses, and transparency through platforms like the Open Science Framework are far more common than they were a decade ago. Journals that once refused to publish replications now promote them, and Registered Reports, where study plans are peer-reviewed before data collection, have gained traction. However, incentives for flashy results still exist, meaning that while progress is real, it is incomplete.     My favorite example is the “thinking outside the box” study, where simply sitting near a cardboard box was said to boost creativity. It highlights how easily tenuous ideas once passed as scientific findings—and how replication might have exposed their weaknesses much sooner.     One additional suggestion that could further address the replication crisis is incentivizing null results. Currently, non-significant findings often go unpublished (“file drawer problem”), skewing the scientific record. Actively rewarding thorough, well-conducted studies—regardless of whether they find dramatic effects—could balance the field toward more genuine discovery. ",AI,Replication
"  I think the suggestions made at the end of the article help us understand that although a lot of experiments conducted in the past, specifically a decade ago or so in this case have potential to have findings that are incorrect or not as conclusive as they seem. The new article suggests that not having a large enough sample size or not sharing the data used in the experiment could “sway” the findings as many researchers might have made mistakes or intentional changes that would affect their final results. I think the article hints at saying that by taking a step back and using a higher population sample its possible to improve the validity and randomization of the experiment and provide more accurate findings. I think my favorite example was the brain scans which clearly showed that years later when reconducting a similar “experiment” that using a larger sample size for brain scan findings resulted in showing that the the initial research/findings were basically “worthless” because of the tiny sample size, but now with a much larger sample the results are more conclusive. I think one suggestion might be for institutions or the government to fund or award research experiments that focus on replicating past experiments to not only check the “validity” but also truly determine if the findings and experiments are replicable as more people would be inclined to do so. From this I believe that we would be able to filter out any incorrect findings earlier on that a decade later and would be able to fix and avoid the “mistakes” we have made in the past. ",Human,Replication
"  The suggestions made in the academic paper, like preregistration and sharing data, definitely helped improve things, but the Asterisk article shows that they weren’t a complete fix. Some researchers are more careful now, and journals have higher standards, but bad practices like p-hacking and selective reporting still happen. It feels like the culture shifted a little, but not as much as people hoped a decade ago.  My favorite example was the ego depletion studies mentioned in the Asterisk article. I remember learning about ego depletion as if it was a fact, but it turns out a lot of those results couldn’t be replicated. It’s crazy how much influence one shaky idea can have on an entire field before people start questioning it.  One suggestion I think could help more is encouraging journals to publish more ""boring"" studies — like replications or studies with null results. Right now, journals mainly want exciting, surprising findings, which pressures researchers to find something dramatic even if it’s not really there. If journals gave more value to solid but less flashy work, it might make the whole system more honest and stable in the long run. ",AI,Replication
"  What stood out to me the most about the rating system in the first article was how misleading the visual compactness can be. I always assumed that the oddly shaped districts were a sign of strongest matches, but the article explains that sometimes those shapes are necessary to ensure fair representation, especially for minority communities. It surprised me how much the rating system penalizes districts that don’t look “normal”, even if they’re drawn for good reasons. If I were in charge, I would focus more on whether a district allows communities to elect candidates of their choice, rather than just how it looks on a map.  As for the second article, I agree with the argument that compact districts aren't always fair. The piece makes a strong case that compactness is often used as a shortcut for fairness, but that doesn’t reflect the real needs of voters. Some communities are spread out, and forcing them into compact shapes that can break them apart and weaken their voting power. I think fairness should be based more on who is represented and less on drawing “pretty” districts. This article made me realize that the conversation around redistricting needs to move beyond aesthetics and start focusing more on actual representation and equity. ",Human,DRA
"  One thing that surprised me the most about the rating system described in the first article was how subjective many of the criteria were. I expected ratings to be more data-driven and standardized, but instead, it seems like a lot of the decisions came down to judgement calls. It made me realize how difficult it can be to design a system that is seen as fair by everyone. If I were making choices for the rating system, I would have tried to find ways to minimize subjective scoring as much as possible, maybe by relying more heavily on clear, measurable outcomes instead of interpretations. It’s crazy how even a system that tries to be natural can end up reflecting bias.  In the second article, I thought the argument that compact districts aren’t necessarily fair made a lot of sense. Before reading it, I always thought more compact districts automatically meant less gerrymandering and more fairness. But the article shows that compactness can still hide unfair outcomes, especially if the underlying populations are uneven. I agree with the idea that fairness should focus more on equal representation and competitive elections rather than just shapes on a map. It definitely challenged my assumptions about redistricting. ",AI,DRA
"  I agree with the argument in the article that compact districts aren’t always fair. It makes sense that the distribution of voters in a state, which the article refers to as ""political geography,"" can affect how fair a district map is, regardless of how compact the districts are. In states with clear urban and rural divides, like Pennsylvania, compact districts can actually make the political map unfair by favoring one party. For example, even though a map might look compact, it can end up giving more seats to Republicans simply because the population is so unevenly distributed. This is something I never thought about before—how compactness, which seems like a fair goal, can sometimes make the map less fair.  On the other hand, the article also shows how a less compact map in a state with uneven political geography might be more fair because it balances out the representation better. I think this is important because it highlights that compactness shouldn’t always be the main factor when creating fair district maps. It’s more about making sure that everyone’s votes count equally, no matter where they live. So, overall, I think the article makes a solid point about how compactness can actually be misleading when it comes to fairness in redistricting. ",Human,DRA
"  In the first article about the DRA rating system, what surprised me most was how subjective and relative the ratings actually are. I expected the ratings to have clear objective benchmarks, but instead, they rely heavily on normalization choices, historical data, and judgment calls about ""good"" and ""bad"" values. If I could change one thing, I would make the thresholds for compactness and proportionality more transparent and adjustable for users who want a more hands-on, customizable evaluation based on their own standards rather than accepting the defaults.  Regarding the second article, I agree with the argument that compactness is not always a politically neutral or fair criterion. Alec Ramsay clearly shows how political geography — the way different voters are clustered — affects whether compact districts produce fair outcomes. For example, in states like Pennsylvania where urban and rural voters are highly polarized, drawing compact districts can unintentionally favor one party, even without intentional gerrymandering. I found it very compelling that remediation (less compact districts) sometimes creates fairer representation. I agree that focusing solely on district shapes is misleading and that fairness should be evaluated based on how accurately districts reflect voter preferences, not just on visual aesthetics or compactness. ",AI,DRA
"  3.1 talks about comparisons of mechanism. It mainly talks about three which are BOS, DA, and TTC. its states "" BOS is used as a natural baseline, since it was actu- ally used for school choice in Boston and New York. DA and TTC are the two lead- ing mechanisms suggested by economists.""  To answer the question, from the article it says that many studies have found that BOS's truth telling rates are lower than under strategy-proof mechanisms. for the DA and TTC the comparison still remains inconclusive. For DA and TTC it says its pretty low  truth rates could be because of the lack of experience.  I read section 4.2 of the article. The most interesting thing I found about this part is that a good amount of matching markets ""are organized in a decentralized manner"". But even if it is , there can still be rules that manages the processes of acceptances.   ",Human,Matching
"In Section 3.1, the paper explains how the form of the matching mechanism significantly affects whether participants truthfully or not report their preferences. More specifically, mechanisms like the Boston mechanism (BM) give students a chance to manipulate the system by strategically ranking schools rather than truthfully. By contrast, the Deferred Acceptance (DA) algorithm encourages greater truthfulness in that it does not penalize students for ranking more desirable schools at the top of their lists—it holds their place until better choices are available, and hence is ""strategy-proof."" This matters because truthfulness results in more efficient and fairer school placements.  I also looked at Section 4, where it discusses DA experiments with real parents. A curious note is that even when people are made aware that DA is strategy-proof, there are others who try to manipulate the system because of fear or ignorance. This says a lot about a disconnect between reality and theory in the real world—just because a system is meant to be fair doesn't always mean that everybody will play by the rules.  Based on these insights, I’d choose a mechanism like DA for school choice. Even if people don’t fully understand the strategy-proof nature of the algorithm, it still protects them from making harmful strategic mistakes and reduces inequity caused by differences in gaming ability or access to information.",AI,Matching
"  The mechanism choice has a considerable impact on truth-telling. Boston Mechanism (BOS) produces truth-telling rates that are lower, with players reporting falsely in manners inconsistent with best strategies. Deferred Acceptance (DA) and Top Trading Cycles (TTC), being strategy-proof mechanisms, encourage greater honest reporting but contrasting truth-telling rates regarding market environments and player experience. DA is stable but has environment-dependent efficiency. While strategy-proof, DA and TTC are not entirely truthful, and real-world implementation problems and lack of experience among participants are suggested to be the reasons.  The most important takeaway is that even though DA is strategy-proof, a large proportion of subjects still report inaccu- rately because of biases such as district school bias, small school bias, and like preferences bias. Based on these results, I would select a mechanism such as DA or TTC but with additional support to prevent misreporting. Moreover, since cognitive ability affects truthful reporting, making the process easier or offering improved decision aids can lead to better outcomes. ",Human,Matching
"  In Section 3.1, the choice of mechanism has a significant impact on truth-telling. Strategy-proof mechanisms like Deferred Acceptance (DA) and Top Trading Cycles (TTC) are designed to make truthful reporting a dominant strategy. However, in practice, even these mechanisms do not guarantee full truthfulness. Many participants still misreport preferences, influenced by misunderstandings, mistrust, or perceived strategic advantages. The results show that although DA promotes more stable outcomes, actual behavior varies depending on participants' comprehension and the surrounding informational environment​.  In Section 3.4, an interesting insight is how biases, risk aversion, and cognitive ability affect reporting strategies. Subjects showed consistent biases like the ""district school bias"" and ""small school bias,"" even when these strategies were suboptimal. Surprisingly, higher cognitive ability correlated with more truthful reporting under TTC but not always under DA​. This suggests that even in strategy-proof mechanisms, factors like fear of loss, complexity of decision-making, and peer influence can heavily distort outcomes.  Based on these insights, a school choice mechanism should combine a strategy-proof system like DA with intensive, clear education efforts for participants. Simplified instructions, examples, and reassurance against strategic fears could significantly improve truth-telling and ultimately lead to fairer, more efficient outcomes. ",AI,Matching
