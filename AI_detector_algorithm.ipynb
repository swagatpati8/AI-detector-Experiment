{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPA6boveyCtT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9481e5fd-b83e-468a-b2de-510379d55f6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True P-Hacked Algorithm Evaluation (Word 'the')\n",
            "Training Data: t-stat=1.4904, p-value=0.1647\n",
            "Test Data: t-stat=0.5382, p-value=0.6172\n",
            "\n",
            "Confusion Matrix on Training Data: {'TP': np.int64(4), 'TN': np.int64(2), 'FP': np.int64(6), 'FN': np.int64(4)}\n",
            "Confusion Matrix on Test Data: {'TP': np.int64(2), 'TN': np.int64(2), 'FP': np.int64(2), 'FN': np.int64(2)}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Load Data\n",
        "train_df = pd.read_csv('Training_data.csv')\n",
        "test_df = pd.read_csv('Test_data.csv')\n",
        "\n",
        "# P-Hacked Heuristic: Counts occurrences of the word 'the'\n",
        "def count_the(text):\n",
        "    return text.lower().split().count('the')\n",
        "\n",
        "# Apply heuristic to both datasets\n",
        "train_df['The_Score'] = train_df['Document'].apply(count_the)\n",
        "test_df['The_Score'] = test_df['Document'].apply(count_the)\n",
        "\n",
        "# Function to Extract Scores for Human and AI\n",
        "def get_scores(df, score_column):\n",
        "    human_scores = df[df['Human OR AI'] == 'Human'][score_column]\n",
        "    ai_scores = df[df['Human OR AI'] == 'AI'][score_column]\n",
        "    return human_scores, ai_scores\n",
        "\n",
        "# Extract scores\n",
        "train_human, train_ai = get_scores(train_df, 'The_Score')\n",
        "test_human, test_ai = get_scores(test_df, 'The_Score')\n",
        "\n",
        "# Statistical Significance Tests (t-test)\n",
        "train_t_stat, train_p_value = ttest_ind(train_human, train_ai, equal_var=False)\n",
        "test_t_stat, test_p_value = ttest_ind(test_human, test_ai, equal_var=False)\n",
        "\n",
        "# Display t-test Results\n",
        "print(\"True P-Hacked Algorithm Evaluation (Word 'the')\")\n",
        "print(f\"Training Data: t-stat={train_t_stat:.4f}, p-value={train_p_value:.4f}\")\n",
        "print(f\"Test Data: t-stat={test_t_stat:.4f}, p-value={test_p_value:.4f}\")\n",
        "\n",
        "# Confusion Matrix Calculation Function\n",
        "def build_confusion_matrix(df, score_column, threshold):\n",
        "    df['Predicted AI'] = df[score_column] > threshold\n",
        "    tp = np.sum((df['Predicted AI'] == True) & (df['Human OR AI'] == 'AI'))\n",
        "    tn = np.sum((df['Predicted AI'] == False) & (df['Human OR AI'] == 'Human'))\n",
        "    fp = np.sum((df['Predicted AI'] == True) & (df['Human OR AI'] == 'Human'))\n",
        "    fn = np.sum((df['Predicted AI'] == False) & (df['Human OR AI'] == 'AI'))\n",
        "    return {'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn}\n",
        "\n",
        "# Determine Threshold: Midpoint Between Human and AI Means\n",
        "threshold = (train_human.mean() + train_ai.mean()) / 2\n",
        "\n",
        "# Build and Display Confusion Matrices\n",
        "train_conf_matrix = build_confusion_matrix(train_df, 'The_Score', threshold)\n",
        "test_conf_matrix = build_confusion_matrix(test_df, 'The_Score', threshold)\n",
        "\n",
        "print(\"\\nConfusion Matrix on Training Data:\", train_conf_matrix)\n",
        "print(\"Confusion Matrix on Test Data:\", test_conf_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import ttest_ind\n",
        "import pandas as pd\n",
        "\n",
        "# Create the alternating labels and ZeroGPT scores manually\n",
        "labels = ['Human', 'AI'] * 8  # 16 items, alternating\n",
        "zerogpt_scores = [\n",
        "    0.00, 1.00,  # 1. Human, 2. AI\n",
        "    0.00, 1.00,  # 3. Human, 4. AI\n",
        "    0.00, 0.70,  # 5. Human, 6. AI\n",
        "    0.00, 1.00,  # 7. Human, 8. AI\n",
        "    0.00, 0.25,  # 9. Human, 10. AI (FN)\n",
        "    0.45, 1.00,  # 11. Human (FP), 12. AI\n",
        "    0.00, 0.21,  # 13. Human, 14. AI (FN)\n",
        "    0.00, 0.50   # 15. Human, 16. AI\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df_train = pd.DataFrame({'Label': labels, 'Score': zerogpt_scores})\n",
        "\n",
        "# Split into human and AI groups\n",
        "human_scores = df_train[df_train['Label'] == 'Human']['Score']\n",
        "ai_scores = df_train[df_train['Label'] == 'AI']['Score']\n",
        "\n",
        "# T-test\n",
        "t_stat, p_val = ttest_ind(human_scores, ai_scores, equal_var=False)\n",
        "\n",
        "# Determine threshold (midpoint between means)\n",
        "threshold = (human_scores.mean() + ai_scores.mean()) / 2\n",
        "\n",
        "# Confusion matrix at 50% prevalence\n",
        "df_train['Predicted'] = df_train['Score'] > threshold\n",
        "tp = np.sum((df_train['Predicted'] == True) & (df_train['Label'] == 'AI'))\n",
        "tn = np.sum((df_train['Predicted'] == False) & (df_train['Label'] == 'Human'))\n",
        "fp = np.sum((df_train['Predicted'] == True) & (df_train['Label'] == 'Human'))\n",
        "fn = np.sum((df_train['Predicted'] == False) & (df_train['Label'] == 'AI'))\n",
        "\n",
        "conf_matrix = {'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn}\n",
        "\n",
        "print(f\"T-statistic (Training): {t_stat:.4f}\")\n",
        "print(f\"P-value (Training): {p_val:.4f}\")\n",
        "print(f\"Threshold (Training): {threshold:.4f}\")\n",
        "print(\"Confusion Matrix (Training):\", conf_matrix)"
      ],
      "metadata": {
        "id": "vOlrNJ3WuLsp",
        "outputId": "b90ef515-e628-4987-f2f1-f95610787bd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T-statistic (Training): -4.8271\n",
            "P-value (Training): 0.0007\n",
            "Threshold (Training): 0.3819\n",
            "Confusion Matrix (Training): {'TP': np.int64(6), 'TN': np.int64(7), 'FP': np.int64(1), 'FN': np.int64(2)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Manually input the ZeroGPT scores for the test data\n",
        "labels_test = ['Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI']  # Alternating labels\n",
        "zerogpt_scores_test = [\n",
        "    0.00, 78.53,  # Human, AI\n",
        "    0.00, 27.00,  # Human, AI (Weird case)\n",
        "    0.00, 100.00,  # Human, AI\n",
        "    0.00, 100.00   # Human, AI\n",
        "]\n",
        "\n",
        "# Create a DataFrame for the test data\n",
        "df_test = pd.DataFrame({'Label': labels_test, 'Score': zerogpt_scores_test})\n",
        "\n",
        "# Split into human and AI groups\n",
        "human_scores_test = df_test[df_test['Label'] == 'Human']['Score']\n",
        "ai_scores_test = df_test[df_test['Label'] == 'AI']['Score']\n",
        "\n",
        "\n",
        "# T-test to check if thereâ€™s a statistically significant difference between Human and AI scores\n",
        "t_stat_test, p_val_test = ttest_ind(human_scores_test, ai_scores_test, equal_var=False)\n",
        "\n",
        "print(f\"T-statistic (Test): {t_stat_test:.4f}\")\n",
        "print(f\"P-value (Test): {p_val_test:.4f}\")\n",
        "\n",
        "\n",
        "# Calculate the threshold based on the mean of both Human and AI scores\n",
        "threshold_test = (human_scores_test.mean() + ai_scores_test.mean()) / 2\n",
        "\n",
        "print(f\"Threshold (Test): {threshold_test:.4f}\")\n",
        "\n",
        "\n",
        "# Apply threshold to predict whether AI or Human\n",
        "df_test['Predicted'] = df_test['Score'] > threshold_test\n",
        "\n",
        "# Confusion matrix\n",
        "tp_test = np.sum((df_test['Predicted'] == True) & (df_test['Label'] == 'AI'))\n",
        "tn_test = np.sum((df_test['Predicted'] == False) & (df_test['Label'] == 'Human'))\n",
        "fp_test = np.sum((df_test['Predicted'] == True) & (df_test['Label'] == 'Human'))\n",
        "fn_test = np.sum((df_test['Predicted'] == False) & (df_test['Label'] == 'AI'))\n",
        "\n",
        "conf_matrix_test = {'TP': tp_test, 'TN': tn_test, 'FP': fp_test, 'FN': fn_test}\n",
        "\n",
        "print(\"Confusion Matrix (Test):\", conf_matrix_test)"
      ],
      "metadata": {
        "id": "ClwLoN5IuRft",
        "outputId": "c068ddc0-691a-41cd-b562-1b6c2d9e7ad9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T-statistic (Test): -4.4354\n",
            "P-value (Test): 0.0213\n",
            "Threshold (Test): 38.1912\n",
            "Confusion Matrix (Test): {'TP': np.int64(3), 'TN': np.int64(4), 'FP': np.int64(0), 'FN': np.int64(1)}\n"
          ]
        }
      ]
    }
  ]
}