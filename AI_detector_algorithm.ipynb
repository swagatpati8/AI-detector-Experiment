{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swagatpati8/AI-detector-Experiment/blob/main/AI_detector_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pcMQ6tCey-Qi",
        "outputId": "b5787f18-65dc-483f-9901-269fd367bbc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZPA6boveyCtT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95356094-2865-4e1f-c26d-9d4dc4ebd0ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True P-Hacked Algorithm Evaluation (Word 'the')\n",
            "Training Data: t-stat=1.4904, p-value=0.1647\n",
            "Test Data: t-stat=0.5382, p-value=0.6172\n",
            "\n",
            "Threshold for 'the' (from training): 10.6875\n",
            "\n",
            "Confusion Matrix on Training Data (50% prevalence): {'TP': np.int64(4), 'TN': np.int64(2), 'FP': np.int64(6), 'FN': np.int64(4)}\n",
            "Confusion Matrix on Test Data (50% prevalence): {'TP': np.int64(2), 'TN': np.int64(2), 'FP': np.int64(2), 'FN': np.int64(2)}\n",
            "\n",
            "Confusion Matrices for Training Data ('the' heuristic) at different prevalences:\n",
            "Prevalence (50%): {'TP': np.int64(5), 'TN': np.int64(3), 'FP': np.int64(5), 'FN': np.int64(3)}\n",
            "Prevalence (25%): {'TP': np.int64(2), 'TN': np.int64(4), 'FP': np.int64(8), 'FN': np.int64(2)}\n",
            "Prevalence (6.125%): {'TP': np.int64(0), 'TN': np.int64(6), 'FP': np.int64(10), 'FN': np.int64(0)}\n",
            "\n",
            "Confusion Matrices for Test Data ('the' heuristic) at different prevalences:\n",
            "Prevalence (50%): {'TP': np.int64(3), 'TN': np.int64(3), 'FP': np.int64(1), 'FN': np.int64(1)}\n",
            "Prevalence (25%): {'TP': np.int64(1), 'TN': np.int64(3), 'FP': np.int64(3), 'FN': np.int64(1)}\n",
            "Prevalence (6.125%): {'TP': np.int64(0), 'TN': np.int64(4), 'FP': np.int64(4), 'FN': np.int64(0)}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Load Data\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/Training_data.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/Test_data.csv')\n",
        "\n",
        "# P-Hacked Heuristic: Counts occurrences of the word 'the'\n",
        "def count_the(text):\n",
        "    return text.lower().split().count('the')\n",
        "\n",
        "# Apply heuristic to both datasets\n",
        "train_df['The_Score'] = train_df['Document'].apply(count_the)\n",
        "test_df['The_Score'] = test_df['Document'].apply(count_the)\n",
        "\n",
        "# Function to Extract Scores for Human and AI\n",
        "def get_scores(df, score_column):\n",
        "    human_scores = df[df['Human OR AI'] == 'Human'][score_column]\n",
        "    ai_scores = df[df['Human OR AI'] == 'AI'][score_column]\n",
        "    return human_scores, ai_scores\n",
        "\n",
        "# Extract scores\n",
        "train_human, train_ai = get_scores(train_df, 'The_Score')\n",
        "test_human, test_ai = get_scores(test_df, 'The_Score')\n",
        "\n",
        "# Statistical Significance Tests (t-test)\n",
        "train_t_stat, train_p_value = ttest_ind(train_human, train_ai, equal_var=False)\n",
        "test_t_stat, test_p_value = ttest_ind(test_human, test_ai, equal_var=False)\n",
        "\n",
        "# Display t-test Results\n",
        "print(\"True P-Hacked Algorithm Evaluation (Word 'the')\")\n",
        "print(f\"Training Data: t-stat={train_t_stat:.4f}, p-value={train_p_value:.4f}\")\n",
        "print(f\"Test Data: t-stat={test_t_stat:.4f}, p-value={test_p_value:.4f}\")\n",
        "\n",
        "# Determine Threshold: Midpoint Between Human and AI Means\n",
        "threshold_the_train = (train_human.mean() + train_ai.mean()) / 2\n",
        "print(f\"\\nThreshold for 'the' (from training): {threshold_the_train:.4f}\")\n",
        "\n",
        "# Confusion Matrix Calculation Function (at 50% prevalence initially)\n",
        "def build_confusion_matrix(df, score_column, threshold):\n",
        "    df['Predicted AI'] = df[score_column] > threshold\n",
        "    tp = np.sum((df['Predicted AI'] == True) & (df['Human OR AI'] == 'AI'))\n",
        "    tn = np.sum((df['Predicted AI'] == False) & (df['Human OR AI'] == 'Human'))\n",
        "    fp = np.sum((df['Predicted AI'] == True) & (df['Human OR AI'] == 'Human'))\n",
        "    fn = np.sum((df['Predicted AI'] == False) & (df['Human OR AI'] == 'AI'))\n",
        "    return {'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn}\n",
        "\n",
        "train_conf_matrix = build_confusion_matrix(train_df, 'The_Score', threshold_the_train)\n",
        "test_conf_matrix = build_confusion_matrix(test_df, 'The_Score', threshold_the_train)\n",
        "\n",
        "print(\"\\nConfusion Matrix on Training Data (50% prevalence):\", train_conf_matrix)\n",
        "print(\"Confusion Matrix on Test Data (50% prevalence):\", test_conf_matrix)\n",
        "\n",
        "# Now run the prevalence-based confusion matrix code\n",
        "def build_confusion_matrix_prevalence(df, score_column, threshold, ai_prevalence):\n",
        "    num_total = len(df)\n",
        "    num_ai = int(num_total * ai_prevalence)\n",
        "    df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    simulated_labels = ['AI'] * num_ai + ['Human'] * (num_total - num_ai)\n",
        "    simulated_df = df_shuffled.copy()\n",
        "    simulated_df['Simulated_Label'] = simulated_labels[:num_total]\n",
        "    simulated_df['Predicted AI'] = simulated_df[score_column] > threshold\n",
        "\n",
        "    tp = np.sum((simulated_df['Predicted AI'] == True) & (simulated_df['Simulated_Label'] == 'AI'))\n",
        "    tn = np.sum((simulated_df['Predicted AI'] == False) & (simulated_df['Simulated_Label'] == 'Human'))\n",
        "    fp = np.sum((simulated_df['Predicted AI'] == True) & (simulated_df['Simulated_Label'] == 'Human'))\n",
        "    fn = np.sum((simulated_df['Predicted AI'] == False) & (simulated_df['Simulated_Label'] == 'AI'))\n",
        "\n",
        "    return {'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn}\n",
        "\n",
        "prevalences = [0.5, 0.25, 0.06125]\n",
        "prevalence_names = [\"50%\", \"25%\", \"6.125%\"]\n",
        "\n",
        "print(\"\\nConfusion Matrices for Training Data ('the' heuristic) at different prevalences:\")\n",
        "for prev, name in zip(prevalences, prevalence_names):\n",
        "    cm_train = build_confusion_matrix_prevalence(train_df.copy(), 'The_Score', threshold_the_train, prev)\n",
        "    print(f\"Prevalence ({name}): {cm_train}\")\n",
        "\n",
        "print(\"\\nConfusion Matrices for Test Data ('the' heuristic) at different prevalences:\")\n",
        "for prev, name in zip(prevalences, prevalence_names):\n",
        "    cm_test = build_confusion_matrix_prevalence(test_df.copy(), 'The_Score', threshold_the_train, prev)\n",
        "    print(f\"Prevalence ({name}): {cm_test}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import ttest_ind\n",
        "import pandas as pd\n",
        "\n",
        "# Create the alternating labels and ZeroGPT scores manually\n",
        "labels = ['Human', 'AI'] * 8  # 16 items, alternating\n",
        "zerogpt_scores = [\n",
        "    0.00, 1.00,  # 1. Human, 2. AI\n",
        "    0.00, 1.00,  # 3. Human, 4. AI\n",
        "    0.00, 0.70,  # 5. Human, 6. AI\n",
        "    0.00, 1.00,  # 7. Human, 8. AI\n",
        "    0.00, 0.25,  # 9. Human, 10. AI (FN)\n",
        "    0.45, 1.00,  # 11. Human (FP), 12. AI\n",
        "    0.00, 0.21,  # 13. Human, 14. AI (FN)\n",
        "    0.00, 0.50   # 15. Human, 16. AI\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df_train = pd.DataFrame({'Label': labels, 'Score': zerogpt_scores})\n",
        "\n",
        "# Split into human and AI groups\n",
        "human_scores = df_train[df_train['Label'] == 'Human']['Score']\n",
        "ai_scores = df_train[df_train['Label'] == 'AI']['Score']\n",
        "\n",
        "# T-test\n",
        "t_stat, p_val = ttest_ind(human_scores, ai_scores, equal_var=False)\n",
        "\n",
        "# Determine threshold (midpoint between means)\n",
        "threshold = (human_scores.mean() + ai_scores.mean()) / 2\n",
        "\n",
        "# Confusion matrix at 50% prevalence\n",
        "df_train['Predicted'] = df_train['Score'] > threshold\n",
        "tp = np.sum((df_train['Predicted'] == True) & (df_train['Label'] == 'AI'))\n",
        "tn = np.sum((df_train['Predicted'] == False) & (df_train['Label'] == 'Human'))\n",
        "fp = np.sum((df_train['Predicted'] == True) & (df_train['Label'] == 'Human'))\n",
        "fn = np.sum((df_train['Predicted'] == False) & (df_train['Label'] == 'AI'))\n",
        "\n",
        "conf_matrix = {'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn}\n",
        "\n",
        "print(f\"T-statistic (Training): {t_stat:.4f}\")\n",
        "print(f\"P-value (Training): {p_val:.4f}\")\n",
        "print(f\"Threshold (Training): {threshold:.4f}\")\n",
        "print(\"Confusion Matrix (Training):\", conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOlrNJ3WuLsp",
        "outputId": "74762147-a6a8-48da-884c-09f1b1a19cd0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T-statistic (Training): -4.8271\n",
            "P-value (Training): 0.0007\n",
            "Threshold (Training): 0.3819\n",
            "Confusion Matrix (Training): {'TP': np.int64(6), 'TN': np.int64(7), 'FP': np.int64(1), 'FN': np.int64(2)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Manually input the ZeroGPT scores for the test data\n",
        "labels_test = ['Human', 'AI', 'Human', 'AI', 'Human', 'AI', 'Human', 'AI']  # Alternating labels\n",
        "zerogpt_scores_test = [\n",
        "    0.00, 78.53,  # Human, AI\n",
        "    0.00, 27.00,  # Human, AI (Weird case)\n",
        "    0.00, 100.00,  # Human, AI\n",
        "    0.00, 100.00   # Human, AI\n",
        "]\n",
        "\n",
        "# Create a DataFrame for the test data\n",
        "df_test = pd.DataFrame({'Label': labels_test, 'Score': zerogpt_scores_test})\n",
        "\n",
        "# Split into human and AI groups\n",
        "human_scores_test = df_test[df_test['Label'] == 'Human']['Score']\n",
        "ai_scores_test = df_test[df_test['Label'] == 'AI']['Score']\n",
        "\n",
        "\n",
        "# T-test to check if there’s a statistically significant difference between Human and AI scores\n",
        "t_stat_test, p_val_test = ttest_ind(human_scores_test, ai_scores_test, equal_var=False)\n",
        "\n",
        "print(f\"T-statistic (Test): {t_stat_test:.4f}\")\n",
        "print(f\"P-value (Test): {p_val_test:.4f}\")\n",
        "\n",
        "\n",
        "# Calculate the threshold based on the mean of both Human and AI scores\n",
        "threshold_test = (human_scores_test.mean() + ai_scores_test.mean()) / 2\n",
        "\n",
        "print(f\"Threshold (Test): {threshold_test:.4f}\")\n",
        "\n",
        "\n",
        "# Apply threshold to predict whether AI or Human\n",
        "df_test['Predicted'] = df_test['Score'] > threshold_test\n",
        "\n",
        "# Confusion matrix\n",
        "tp_test = np.sum((df_test['Predicted'] == True) & (df_test['Label'] == 'AI'))\n",
        "tn_test = np.sum((df_test['Predicted'] == False) & (df_test['Label'] == 'Human'))\n",
        "fp_test = np.sum((df_test['Predicted'] == True) & (df_test['Label'] == 'Human'))\n",
        "fn_test = np.sum((df_test['Predicted'] == False) & (df_test['Label'] == 'AI'))\n",
        "\n",
        "conf_matrix_test = {'TP': tp_test, 'TN': tn_test, 'FP': fp_test, 'FN': fn_test}\n",
        "\n",
        "print(\"Confusion Matrix (Test):\", conf_matrix_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClwLoN5IuRft",
        "outputId": "3374d04d-a8c5-451a-e75b-6721447d90c4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T-statistic (Test): -4.4354\n",
            "P-value (Test): 0.0213\n",
            "Threshold (Test): 38.1912\n",
            "Confusion Matrix (Test): {'TP': np.int64(3), 'TN': np.int64(4), 'FP': np.int64(0), 'FN': np.int64(1)}\n"
          ]
        }
      ]
    }
  ]
}